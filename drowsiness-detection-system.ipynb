{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1461668,"sourceType":"datasetVersion","datasetId":857148},{"sourceId":2935731,"sourceType":"datasetVersion","datasetId":839168}],"dockerImageVersionId":30302,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Drowsiness Detection System\n\n- Drowsy Driving is a deadly combination of driving and sleepiness. \n\n- The number of road accidents due to Drowsy Driving is increasing at an alarming rate worldwide. \n\n- Not having a proper sleep is the main reason behind drowsiness while driving. However, other reasons like sleep disorders, medication, alcohol consumption, or driving during night shifts can also cause drowsiness while driving.\n","metadata":{}},{"cell_type":"markdown","source":"# Loading data and libraries","metadata":{}},{"cell_type":"code","source":"!pip install mediapipe\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport cv2\nimport numpy as np\nimport mediapipe as mp\nimport os\nimport shutil\nimport matplotlib.pyplot as plt\nimport mediapipe as mp\n \nmp_facemesh = mp.solutions.face_mesh\nmp_drawing  = mp.solutions.drawing_utils\ndenormalize_coordinates = mp_drawing._normalized_to_pixel_coordinates\n \n%matplotlib inline","metadata":{"id":"xFw2LMhLPgE4","outputId":"4b5c45e0-d207-4217-fb8b-b6f8d0519c21","execution":{"iopub.status.busy":"2024-02-22T21:52:55.348963Z","iopub.execute_input":"2024-02-22T21:52:55.349645Z","iopub.status.idle":"2024-02-22T21:53:15.666560Z","shell.execute_reply.started":"2024-02-22T21:52:55.349562Z","shell.execute_reply":"2024-02-22T21:53:15.665641Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting mediapipe\n  Downloading mediapipe-0.9.0.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.0/33.0 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hRequirement already satisfied: opencv-contrib-python in /opt/conda/lib/python3.7/site-packages (from mediapipe) (4.5.4.60)\nRequirement already satisfied: attrs>=19.1.0 in /opt/conda/lib/python3.7/site-packages (from mediapipe) (21.4.0)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.7/site-packages (from mediapipe) (0.15.0)\nRequirement already satisfied: protobuf<4,>=3.11 in /opt/conda/lib/python3.7/site-packages (from mediapipe) (3.19.4)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from mediapipe) (3.5.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from mediapipe) (1.21.6)\nCollecting flatbuffers>=2.0\n  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from absl-py->mediapipe) (1.15.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->mediapipe) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->mediapipe) (9.1.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->mediapipe) (4.33.3)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->mediapipe) (0.11.0)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib->mediapipe) (2.8.2)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->mediapipe) (1.4.3)\nRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->mediapipe) (3.0.9)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib->mediapipe) (4.1.1)\nInstalling collected packages: flatbuffers, mediapipe\n  Attempting uninstall: flatbuffers\n    Found existing installation: flatbuffers 1.12\n    Uninstalling flatbuffers-1.12:\n      Successfully uninstalled flatbuffers-1.12\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, which is not installed.\ntensorflow 2.6.4 requires flatbuffers~=1.12.0, but you have flatbuffers 23.5.26 which is incompatible.\ntensorflow 2.6.4 requires h5py~=3.1.0, but you have h5py 3.7.0 which is incompatible.\ntensorflow 2.6.4 requires numpy~=1.19.2, but you have numpy 1.21.6 which is incompatible.\ntensorflow 2.6.4 requires tensorboard<2.7,>=2.6.0, but you have tensorboard 2.10.1 which is incompatible.\ntensorflow 2.6.4 requires typing-extensions<3.11,>=3.7, but you have typing-extensions 4.1.1 which is incompatible.\ntensorflow-transform 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\ntensorflow-serving-api 2.9.0 requires tensorflow<3,>=2.9.0, but you have tensorflow 2.6.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed flatbuffers-23.5.26 mediapipe-0.9.0.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"os.makedirs('./Fatigue Subjects')\nos.makedirs('./Active Subjects')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","id":"vKOwYE7YPR-k","execution":{"iopub.status.busy":"2024-02-22T21:53:15.668419Z","iopub.execute_input":"2024-02-22T21:53:15.668718Z","iopub.status.idle":"2024-02-22T21:53:15.673615Z","shell.execute_reply.started":"2024-02-22T21:53:15.668689Z","shell.execute_reply":"2024-02-22T21:53:15.672603Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Image preprocessing : \n### our preprocessing will include\n- Detecting faces from images\n- Drawing landmarks on our images to increase performance\n- Resizing our images \n- LabelEncoding\n- Image Augmantation","metadata":{}},{"cell_type":"markdown","source":"# Landmarks :\nWe will use mediapipe to draw landmarks on our images after detecting faces and croping them","metadata":{}},{"cell_type":"code","source":"# Landmark points corresponding to left eye\nall_left_eye_idxs = list(mp_facemesh.FACEMESH_LEFT_EYE)\n# flatten and remove duplicates\nall_left_eye_idxs = set(np.ravel(all_left_eye_idxs)) \n \n# Landmark points corresponding to right eye\nall_right_eye_idxs = list(mp_facemesh.FACEMESH_RIGHT_EYE)\nall_right_eye_idxs = set(np.ravel(all_right_eye_idxs))\n \n# Combined for plotting - Landmark points for both eye\nall_idxs = all_left_eye_idxs.union(all_right_eye_idxs)\n \n# The chosen 12 points:   P1,  P2,  P3,  P4,  P5,  P6\nchosen_left_eye_idxs  = [362, 385, 387, 263, 373, 380]\nchosen_right_eye_idxs = [33,  160, 158, 133, 153, 144]\nall_chosen_idxs = chosen_left_eye_idxs + chosen_right_eye_idxs","metadata":{"id":"KSgkzCv5cX4p","execution":{"iopub.status.busy":"2024-02-22T21:53:15.674836Z","iopub.execute_input":"2024-02-22T21:53:15.675204Z","iopub.status.idle":"2024-02-22T21:53:15.685645Z","shell.execute_reply.started":"2024-02-22T21:53:15.675166Z","shell.execute_reply":"2024-02-22T21:53:15.684801Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"IMG_SIZE=145\ni=0\ndef draw(\n    *,n=i,\n    img_dt,cat,\n    img_eye_lmks=None,\n    img_eye_lmks_chosen=None,\n    face_landmarks=None,\n    ts_thickness=1,\n    ts_circle_radius=2,\n    lmk_circle_radius=3,\n    name=\"1\",\n):\n    # For plotting Face Tessellation\n    image_drawing_tool = img_dt \n     \n     # For plotting all eye landmarks\n    image_eye_lmks = img_dt.copy() if img_eye_lmks is None else img_eye_lmks\n     \n    # For plotting chosen eye landmarks\n    img_eye_lmks_chosen = img_dt.copy() if img_eye_lmks_chosen is None else img_eye_lmks_chosen\n \n    # Initializing drawing utilities for plotting face mesh tessellation\n    connections_drawing_spec = mp_drawing.DrawingSpec(\n        thickness=ts_thickness, \n        circle_radius=ts_circle_radius, \n        color=(255, 255, 255)\n    )\n\n \n    # Draw landmarks on face using the drawing utilities.\n    mp_drawing.draw_landmarks(\n        image=image_drawing_tool,\n        landmark_list=face_landmarks,\n        connections=mp_facemesh.FACEMESH_TESSELATION,\n        landmark_drawing_spec=None,\n        connection_drawing_spec=connections_drawing_spec,\n    )\n \n    # Get the object which holds the x, y, and z coordinates for each landmark\n    landmarks = face_landmarks.landmark\n \n    # Iterate over all landmarks.\n    # If the landmark_idx is present in either all_idxs or all_chosen_idxs,\n    # get the denormalized coordinates and plot circles at those coordinates.\n \n    for landmark_idx, landmark in enumerate(landmarks):\n        if landmark_idx in all_idxs:\n            pred_cord = denormalize_coordinates(landmark.x, \n                                                landmark.y, \n                                                imgW, imgH)\n            cv2.circle(image_eye_lmks, \n                       pred_cord, \n                       lmk_circle_radius, \n                       (255, 255, 255), \n                       -1\n                       )\n \n        if landmark_idx in all_chosen_idxs:\n            pred_cord = denormalize_coordinates(landmark.x, \n                                                landmark.y, \n                                                imgW, imgH)\n            cv2.circle(img_eye_lmks_chosen, \n                       pred_cord, \n                       lmk_circle_radius, \n                       (255, 255, 255), \n                       -1\n                       )\n \n    if cat=='Fatigue Subjects':\n        cv2.imwrite(str('./Fatigue Subjects/'+str(n)+'.jpg'), image_drawing_tool)\n    else:\n        cv2.imwrite(str('./Active Subjects/'+str(n)+'.jpg'), image_drawing_tool)\n\n    resized_array = cv2.resize(image_drawing_tool, (IMG_SIZE, IMG_SIZE)) \n    return resized_array","metadata":{"id":"Dm3ZA9ypcXv5","execution":{"iopub.status.busy":"2024-02-22T21:53:15.687069Z","iopub.execute_input":"2024-02-22T21:53:15.687447Z","iopub.status.idle":"2024-02-22T21:53:15.701637Z","shell.execute_reply.started":"2024-02-22T21:53:15.687417Z","shell.execute_reply":"2024-02-22T21:53:15.700817Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"imgH, imgW, _=0,0,0 \ndef landmarks(image,category,i):\n    resized_array=[]\n    IMG_SIZE = 145\n    image = np.ascontiguousarray(image)\n    imgH, imgW, _ = image.shape\n                            \n     # Running inference using static_image_mode \n    with mp_facemesh.FaceMesh(\n        static_image_mode=True,         # Default=False\n        max_num_faces=1,                # Default=1\n        refine_landmarks=False,         # Default=False\n        min_detection_confidence=0.5,   # Default=0.5\n        min_tracking_confidence= 0.5,) as face_mesh:\n\n        results = face_mesh.process(image)\n\n        # If detections are available.\n        if results.multi_face_landmarks:  \n            for face_id, face_landmarks in enumerate(results.multi_face_landmarks):\n                resized_array= draw(img_dt=image.copy(), cat=category, n=i,face_landmarks=face_landmarks)\n    return resized_array","metadata":{"id":"b5ln3QUWerf5","execution":{"iopub.status.busy":"2024-02-22T21:53:15.704611Z","iopub.execute_input":"2024-02-22T21:53:15.704975Z","iopub.status.idle":"2024-02-22T21:53:15.715042Z","shell.execute_reply.started":"2024-02-22T21:53:15.704948Z","shell.execute_reply":"2024-02-22T21:53:15.714123Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def face_for_yawn(direc=\"../input/drowsiness-prediction-dataset/0 FaceImages\", face_cas_path=\"../input/prediction-images/haarcascade_frontalface_default.xml\"):\n    yaw_no=[]\n    i=1\n    IMG_SIZE = 145\n    categories = [\"Fatigue Subjects\", \"Active Subjects\"]\n    for category in categories:\n        path_link = os.path.join(direc, category)\n        class_num1 = categories.index(category)\n        print(class_num1)\n        for image in os.listdir(path_link):\n            image_array = cv2.imread(os.path.join(path_link, image), cv2.IMREAD_COLOR)\n            face_cascade = cv2.CascadeClassifier(face_cas_path)\n            faces = face_cascade.detectMultiScale(image_array, 1.3, 5)\n            for (x, y, w, h) in faces:\n                img = cv2.rectangle(image_array, (x, y), (x+w, y+h), (0, 255, 0), 2)\n                roi_color = img[y:y+h, x:x+w]\n                land_face_array=landmarks(roi_color,category,i)\n                yaw_no.append([land_face_array, class_num1])\n                i=i+1\n    return yaw_no\nyawn_no_yawn = face_for_yawn()","metadata":{"id":"Dck2gCV-PR-p","outputId":"ebe014a9-4c11-465c-b2fe-415978993657","execution":{"iopub.status.busy":"2024-02-22T21:53:15.716255Z","iopub.execute_input":"2024-02-22T21:53:15.716594Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"0\n","output_type":"stream"},{"name":"stderr","text":"INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n","output_type":"stream"}]},{"cell_type":"code","source":"dir_path = r'./Active Subjects'\nprint(\"Number of Active images :\")\nprint(len([entry for entry in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path, entry))]))","metadata":{"id":"WEwfoTWB98w0","outputId":"60a0e3f1-9a6a-46d4-971b-f4655aa29964","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dir_path = r'./Fatigue Subjects'\nprint(\"Number of Fatigue images :\")\nprint(len([entry for entry in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path, entry))]))","metadata":{"id":"E1tHP_FL98nF","outputId":"0ae4c09e-fa06-4ce9-c869-6580db6edc05","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Our images will be like this:","metadata":{}},{"cell_type":"code","source":"categories = [\"Fatigue Subjects\", \"Active Subjects\"]\nfor category in categories:\n  for idx, img in enumerate(os.listdir(f'./{category}')):\n      if idx > 5:\n        break\n      img_file = cv2.imread(f'./{category}/{img}')\n      plt.imshow(img_file)\n      plt.show()\n      plt.close()","metadata":{"id":"UCgijSq_zuW7","outputId":"259b845b-5cfa-4843-d2f6-73013a8acead","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Resizing images","metadata":{}},{"cell_type":"code","source":"import os\nimport time\ndef face_for_yawn(direc=\"./\"):\n    yaw_no=[]\n    i=1\n    IMG_SIZE = 145\n    categories = [\"Fatigue Subjects\", \"Active Subjects\"]\n    for category in categories:\n        path_link = os.path.join(direc, category)\n        class_num1 = categories.index(category)\n        print(class_num1)\n        for image in os.listdir(path_link):\n            image_array = cv2.imread(os.path.join(path_link, image), cv2.IMREAD_COLOR)\n            resized_array = cv2.resize(image_array, (IMG_SIZE, IMG_SIZE))\n            yaw_no.append([resized_array, class_num1])\n                #print('image face number '+str(i))\n                #i=i+1\n    return yaw_no\nyawn_no_yawn = face_for_yawn()","metadata":{"id":"0tHKdRF8PR-r","outputId":"7ce60be0-da17-4900-a847-bb626219f978","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## separate label and features","metadata":{"id":"3isUEA8XPR-s"}},{"cell_type":"code","source":"X = []\ny = []\nfor feature, label in yawn_no_yawn:\n    X.append(feature)\n    y.append(label)","metadata":{"id":"mVRgroJEPR-s","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reshape the array","metadata":{"id":"kOWjQcuaPR-s"}},{"cell_type":"code","source":"X = np.array(X)\nX = X.reshape(-1, 145, 145, 3)","metadata":{"id":"vDpba9fEPR-s","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LabelEncoder","metadata":{"id":"9rOouR4tPR-s"}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nlabel_bin = LabelEncoder()\ny = label_bin.fit_transform(y)\ny = np.array(y)","metadata":{"id":"uR7V1r8iPR-s","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Splitting","metadata":{"id":"qGxNWmYwPR-t"}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nseed = 42\ntest_size = 0.20\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=seed, test_size=test_size)","metadata":{"id":"HCYAjIgTPR-t","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(X_test)","metadata":{"id":"5umUs7eMPR-t","outputId":"5aad438a-d2bf-4ac4-b960-1b80f544d959","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(X_train)","metadata":{"id":"6bzaKKbTzpmQ","outputId":"00a5cb3d-dee8-4458-9b80-e10cf8bcae71","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### import some dependencies","metadata":{"id":"JwaNP1OePR-u"}},{"cell_type":"code","source":"from tensorflow.keras.layers import Input, Lambda, Dense, Flatten, Conv2D, MaxPooling2D, Dropout\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.models import Sequential\nfrom keras.preprocessing.image import ImageDataGenerator\nimport tensorflow as tf","metadata":{"id":"sTgEHYiGPR-u","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Augmentation","metadata":{"id":"LOdCyv1APR-v"}},{"cell_type":"code","source":"train_generator = ImageDataGenerator(rescale=1/255, zoom_range=0.2, horizontal_flip=True, rotation_range=30)\ntest_generator = ImageDataGenerator(rescale=1/255)\n\ntrain_generator = train_generator.flow(np.array(X_train), y_train, shuffle=False)\ntest_generator = test_generator.flow(np.array(X_test), y_test, shuffle=False)","metadata":{"id":"B88AD-JVPR-v","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{"id":"griSUS_zPR-v"}},{"cell_type":"code","source":"# Model Elsafty 1\nfrom keras.layers import BatchNormalization\nmodel = tf.keras.models.Sequential()\n# Note the input shape is the desired size of the image 145 x 145 with 3 bytes color\n# This is the first convolution\nmodel.add(Conv2D(16, 3, activation='relu', input_shape=X_train.shape[1:]))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D())\ntf.keras.layers.Dropout(0.1)\n# The second convolution\nmodel.add(Conv2D(32, 5, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D())\ntf.keras.layers.Dropout(0.1)\n# The third convolution\nmodel.add(Conv2D(64, 10, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D())\ntf.keras.layers.Dropout(0.1)\n# The fourth convolution\nmodel.add(Conv2D(128, 12, activation='relu'))\nmodel.add(BatchNormalization())\n\n# Flatten the results to feed into a DNN\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(64, activation='relu'))\n# Only 1 output neuron.\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss=\"binary_crossentropy\", metrics=[\"accuracy\"], optimizer=\"adam\")\nmodel.summary()","metadata":{"id":"m_cVcm3mWNBJ","outputId":"2f5fa158-5401-405f-82ac-637e03772d9b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(train_generator, epochs=70, validation_data=test_generator)","metadata":{"id":"5XJ5vd7BoYLz","outputId":"02058add-e33b-4c12-fdef-819485103d8a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy = history.history['accuracy']\nval_accuracy = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(len(accuracy))\n\nplt.plot(epochs, accuracy, \"b\", label=\"trainning accuracy\")\nplt.plot(epochs, val_accuracy, \"r\", label=\"validation accuracy\")\nplt.legend()\nplt.show()\n\nplt.plot(epochs, loss, \"b\", label=\"trainning loss\")\nplt.plot(epochs, val_loss, \"r\", label=\"validation loss\")\nplt.legend()\nplt.show()","metadata":{"id":"r3jV3Uo3GC5f","outputId":"a817282e-875c-4f62-b4a4-a3a5d3e16067","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# You can evaluate or predict on a dataset.\nprint(\"Evaluate\")\nresult = model.evaluate(test_generator)\ndict(zip(model.metrics_names, result))","metadata":{"id":"26AN1kQWPFjk","outputId":"62b0d0cb-40ca-4989-cd6e-ab559e3a782b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('my_model.h5')\n#model = tf.keras.models.load_model('my_model.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from mlxtend.plotting import plot_confusion_matrix\nimport matplotlib.pyplot as plt\nimport numpy as np\nbinary1 = np.array([[3427,252],[44,2331]])\nfig, ax = plot_confusion_matrix(conf_mat=binary1,show_absolute=True,\n                                show_normed=True,\n                                colorbar=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualizing our CNN architecture","metadata":{}},{"cell_type":"code","source":"from keras.utils.vis_utils import plot_model\nplot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)\nfrom IPython.display import Image\nImage(\"model.png\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install visualkeras\nimport visualkeras\n\nvisualkeras.layered_view(model).show() # display using your system viewer\nvisualkeras.layered_view(model, to_file='output.png') # write to disk\nvisualkeras.layered_view(model, to_file='output.png').show() # write and show\n\nvisualkeras.layered_view(model,legend=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install keras_sequential_ascii\nfrom keras_sequential_ascii import keras2ascii\nkeras2ascii(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}